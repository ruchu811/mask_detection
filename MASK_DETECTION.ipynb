{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "89i-XfxXodTi"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeQE4wAN1LhQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_Vc_W25DrN-5"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2boCBjljuixC",
    "outputId": "11cedd91-76d1-4989-b703-572a93744676"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"Project_Mask_Detection.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bjRHsDYNtZmq"
   },
   "outputs": [],
   "source": [
    "def resize_all(src, pklname, include, width=300, height=None):\n",
    "    \"\"\"\n",
    "    load images from path, resize them and write them as arrays to a dictionary, \n",
    "    together with labels and metadata. The dictionary is written to a pickle file \n",
    "    named '{pklname}_{width}x{height}px.pkl'.\n",
    "     \n",
    "    Parameter\n",
    "    ---------\n",
    "    src: str\n",
    "        path to data\n",
    "    pklname: str\n",
    "        path to output file\n",
    "    width: int\n",
    "        target width of the image in pixels\n",
    "    include: set[str]\n",
    "        set containing str\n",
    "    \"\"\"\n",
    "     \n",
    "    height = height if height is not None else width\n",
    "     \n",
    "    data = dict()\n",
    "    data['description'] = 'resized ({0}x{1})animal images in rgb'.format(int(width), int(height))\n",
    "    data['label'] = []\n",
    "    data['filename'] = []\n",
    "    data['data'] = []   \n",
    "     \n",
    "    pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "    # read all images in PATH, resize and write to DESTINATION_PATH\n",
    "    for subdir in os.listdir(src):\n",
    "        if subdir in include:\n",
    "            print(subdir)\n",
    "            current_path = os.path.join(src, subdir)\n",
    " \n",
    "            for file in os.listdir(current_path):\n",
    "                if file[-3:] in {'jpg', 'png'}:\n",
    "                    im = imread(os.path.join(current_path, file))\n",
    "                    im = resize(im, (width, height)) #[:,:,::-1]\n",
    "                    data['label'].append(int(subdir[:1]))\n",
    "                    data['filename'].append(file)\n",
    "                    data['data'].append(im)\n",
    " \n",
    "        joblib.dump(data, pklname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDPHgbscvIIv",
    "outputId": "0b8621d1-9689-45ff-850e-2446d7c8a419"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "resize_all(src='Project_Mask_Detection/maskdata/maskdata/train', pklname='train_set', include={'1','0'}, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oW_YDzRX0_Tf",
    "outputId": "02e168eb-103b-4642-edf6-a4baf91139bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "resize_all(src='Project_Mask_Detection/maskdata/maskdata/test', pklname='test_set', include={'1','0'}, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsA8OA5yJAek",
    "outputId": "73befd37-3419-4764-b881-00871c1b3311"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples:  616\nkeys:  ['description', 'label', 'filename', 'data']\ndescription:  resized (300x300)animal images in rgb\nimage shape:  (300, 300, 3)\nlabels: [0 1]\n(616, 1)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data = joblib.load(f'train_set_300x300px.pkl')\n",
    "X_train = np.array(data['data'])\n",
    "Y_train = np.array(data['label'])\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "print('number of samples: ', len(data['data']))\n",
    "print('keys: ', list(data.keys()))\n",
    "print('description: ', data['description'])\n",
    "print('image shape: ', data['data'][0].shape)\n",
    "print('labels:', np.unique(Y_train))\n",
    " \n",
    "Counter(data['label'])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "Y_train[275]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cXf7XeWYldY",
    "outputId": "f43a5400-f348-4bb7-bb28-7fc79ec9dbd6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples:  198\nkeys:  ['description', 'label', 'filename', 'data']\ndescription:  resized (300x300)animal images in rgb\nimage shape:  (300, 300, 3)\nlabels: [0 1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(198, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data_test = joblib.load(f'test_set_300x300px.pkl')\n",
    " \n",
    "X_test = np.array(data_test['data'])\n",
    "Y_test = data_test['label']\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test = Y_test.reshape(-1,1)\n",
    "print('number of samples: ', len(data_test['data']))\n",
    "print('keys: ', list(data_test.keys()))\n",
    "print('description: ', data_test['description'])\n",
    "print('image shape: ', data_test['data'][0].shape)\n",
    "print('labels:', np.unique(Y_test))\n",
    "\n",
    "Counter(data_test['label'])\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QQjzHd6TKkjH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "uqDHmxSaaBaq"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor= 'val_loss',mode ='min',verbose=1, patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orWL_21NPt_o",
    "outputId": "92b35cb5-863f-489b-8ac5-f4c500010242"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "31/31 [==============================] - 114s 4s/step - loss: 0.1177 - acc: 0.9692 - val_loss: 0.1762 - val_acc: 0.9293\n",
      "INFO:tensorflow:Assets written to: model-001.model\\assets\n",
      "Epoch 2/30\n",
      "31/31 [==============================] - 117s 4s/step - loss: 0.0666 - acc: 0.9773 - val_loss: 0.1707 - val_acc: 0.9192\n",
      "INFO:tensorflow:Assets written to: model-002.model\\assets\n",
      "Epoch 3/30\n",
      "31/31 [==============================] - 115s 4s/step - loss: 0.4482 - acc: 0.9610 - val_loss: 0.2985 - val_acc: 0.8939\n",
      "Epoch 4/30\n",
      "31/31 [==============================] - 117s 4s/step - loss: 0.0483 - acc: 0.9838 - val_loss: 0.3508 - val_acc: 0.9141\n",
      "Epoch 5/30\n",
      "31/31 [==============================] - 117s 4s/step - loss: 0.0288 - acc: 0.9886 - val_loss: 0.2696 - val_acc: 0.9495\n",
      "Epoch 6/30\n",
      "31/31 [==============================] - 111s 4s/step - loss: 0.2335 - acc: 0.9610 - val_loss: 0.1815 - val_acc: 0.9495\n",
      "Epoch 7/30\n",
      "31/31 [==============================] - 116s 4s/step - loss: 0.0211 - acc: 0.9870 - val_loss: 0.1973 - val_acc: 0.9444\n",
      "Epoch 8/30\n",
      "31/31 [==============================] - 113s 4s/step - loss: 0.2487 - acc: 0.9756 - val_loss: 0.1636 - val_acc: 0.9596\n",
      "INFO:tensorflow:Assets written to: model-008.model\\assets\n",
      "Epoch 9/30\n",
      "31/31 [==============================] - 110s 4s/step - loss: 0.0088 - acc: 0.9935 - val_loss: 0.2208 - val_acc: 0.9343\n",
      "Epoch 10/30\n",
      "31/31 [==============================] - 114s 4s/step - loss: 0.1043 - acc: 0.9789 - val_loss: 0.1684 - val_acc: 0.9596\n",
      "Epoch 11/30\n",
      "31/31 [==============================] - 103s 3s/step - loss: 0.0303 - acc: 0.9886 - val_loss: 0.2742 - val_acc: 0.9293\n",
      "Epoch 12/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0326 - acc: 0.9935 - val_loss: 0.6470 - val_acc: 0.8788\n",
      "Epoch 13/30\n",
      "31/31 [==============================] - 105s 3s/step - loss: 0.0409 - acc: 0.9919 - val_loss: 0.1276 - val_acc: 0.9596\n",
      "INFO:tensorflow:Assets written to: model-013.model\\assets\n",
      "Epoch 14/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.4231 - acc: 0.9740 - val_loss: 0.1815 - val_acc: 0.9394\n",
      "Epoch 15/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0483 - acc: 0.9838 - val_loss: 0.4316 - val_acc: 0.9141\n",
      "Epoch 16/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.3128 - acc: 0.9740 - val_loss: 0.2856 - val_acc: 0.9141\n",
      "Epoch 17/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0054 - acc: 0.9968 - val_loss: 0.2289 - val_acc: 0.9343\n",
      "Epoch 18/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0457 - acc: 0.9886 - val_loss: 0.1854 - val_acc: 0.9495\n",
      "Epoch 19/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0864 - acc: 0.9838 - val_loss: 0.2416 - val_acc: 0.9394\n",
      "Epoch 20/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0126 - acc: 0.9968 - val_loss: 0.9279 - val_acc: 0.8788\n",
      "Epoch 21/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0165 - acc: 0.9951 - val_loss: 0.4001 - val_acc: 0.9192\n",
      "Epoch 22/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0321 - acc: 0.9919 - val_loss: 0.2135 - val_acc: 0.9394\n",
      "Epoch 23/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0251 - acc: 0.9968 - val_loss: 0.2609 - val_acc: 0.9242\n",
      "Epoch 24/30\n",
      "31/31 [==============================] - 107s 3s/step - loss: 0.0956 - acc: 0.9789 - val_loss: 0.1864 - val_acc: 0.9444\n",
      "Epoch 25/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0240 - acc: 0.9935 - val_loss: 0.2992 - val_acc: 0.9394\n",
      "Epoch 26/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.0341 - acc: 0.9935 - val_loss: 0.5000 - val_acc: 0.9141\n",
      "Epoch 27/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0441 - acc: 0.9919 - val_loss: 0.2116 - val_acc: 0.9444\n",
      "Epoch 28/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0126 - acc: 0.9951 - val_loss: 0.3068 - val_acc: 0.9192\n",
      "Epoch 29/30\n",
      "31/31 [==============================] - 101s 3s/step - loss: 0.1485 - acc: 0.9773 - val_loss: 0.5272 - val_acc: 0.9091\n",
      "Epoch 30/30\n",
      "31/31 [==============================] - 100s 3s/step - loss: 0.0130 - acc: 0.9951 - val_loss: 0.8332 - val_acc: 0.9141\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22bb6fd7b38>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "model.fit(x=X_train, y=Y_train, validation_data=(X_test,Y_test), batch_size=20, epochs = 30,callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'put on your mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "  #  model = load_model('model-021.model')\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(300,300))\n",
    "        reshaped=np.reshape(resized,(1,300,300,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MASK_DETECTION.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python368jvsc74a57bd0a54b4aca33c11560c67db02cf27fd91ac8e268774d0d2f50d2d73e32adaac664",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a54b4aca33c11560c67db02cf27fd91ac8e268774d0d2f50d2d73e32adaac664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}