{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "89i-XfxXodTi"
   },
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EeQE4wAN1LhQ"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_Vc_W25DrN-5"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2boCBjljuixC",
    "outputId": "11cedd91-76d1-4989-b703-572a93744676"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "file_name = \"Project_Mask_Detection.zip\"\n",
    "\n",
    "with ZipFile(file_name, 'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bjRHsDYNtZmq"
   },
   "outputs": [],
   "source": [
    "def resize_all(src, pklname, include, width=300, height=None):\n",
    "    \"\"\"\n",
    "    load images from path, resize them and write them as arrays to a dictionary, \n",
    "    together with labels and metadata. The dictionary is written to a pickle file \n",
    "    named '{pklname}_{width}x{height}px.pkl'.\n",
    "     \n",
    "    Parameter\n",
    "    ---------\n",
    "    src: str\n",
    "        path to data\n",
    "    pklname: str\n",
    "        path to output file\n",
    "    width: int\n",
    "        target width of the image in pixels\n",
    "    include: set[str]\n",
    "        set containing str\n",
    "    \"\"\"\n",
    "     \n",
    "    height = height if height is not None else width\n",
    "     \n",
    "    data = dict()\n",
    "    data['description'] = 'resized ({0}x{1})animal images in rgb'.format(int(width), int(height))\n",
    "    data['label'] = []\n",
    "    data['filename'] = []\n",
    "    data['data'] = []   \n",
    "     \n",
    "    pklname = f\"{pklname}_{width}x{height}px.pkl\"\n",
    " \n",
    "    # read all images in PATH, resize and write to DESTINATION_PATH\n",
    "    for subdir in os.listdir(src):\n",
    "        if subdir in include:\n",
    "            print(subdir)\n",
    "            current_path = os.path.join(src, subdir)\n",
    " \n",
    "            for file in os.listdir(current_path):\n",
    "                if file[-3:] in {'jpg', 'png'}:\n",
    "                    im = imread(os.path.join(current_path, file))\n",
    "                    im = resize(im, (width, height)) #[:,:,::-1]\n",
    "                    data['label'].append(int(subdir[:1]))\n",
    "                    data['filename'].append(file)\n",
    "                    data['data'].append(im)\n",
    " \n",
    "        joblib.dump(data, pklname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDPHgbscvIIv",
    "outputId": "0b8621d1-9689-45ff-850e-2446d7c8a419"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "resize_all(src='Project_Mask_Detection/maskdata/maskdata/train', pklname='train_set', include={'1','0'}, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oW_YDzRX0_Tf",
    "outputId": "02e168eb-103b-4642-edf6-a4baf91139bf"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "resize_all(src='Project_Mask_Detection/maskdata/maskdata/test', pklname='test_set', include={'1','0'}, width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsA8OA5yJAek",
    "outputId": "73befd37-3419-4764-b881-00871c1b3311"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples:  616\nkeys:  ['description', 'label', 'filename', 'data']\ndescription:  resized (300x300)animal images in rgb\nimage shape:  (300, 300, 3)\nlabels: [0 1]\n(616, 1)\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "data = joblib.load(f'train_set_300x300px.pkl')\n",
    "X_train = np.array(data['data'])\n",
    "Y_train = np.array(data['label'])\n",
    "Y_train = Y_train.reshape(-1,1)\n",
    "print('number of samples: ', len(data['data']))\n",
    "print('keys: ', list(data.keys()))\n",
    "print('description: ', data['description'])\n",
    "print('image shape: ', data['data'][0].shape)\n",
    "print('labels:', np.unique(Y_train))\n",
    " \n",
    "Counter(data['label'])\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "Y_train[274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1cXf7XeWYldY",
    "outputId": "f43a5400-f348-4bb7-bb28-7fc79ec9dbd6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of samples:  198\nkeys:  ['description', 'label', 'filename', 'data']\ndescription:  resized (300x300)animal images in rgb\nimage shape:  (300, 300, 3)\nlabels: [0 1]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(198, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "data_test = joblib.load(f'test_set_300x300px.pkl')\n",
    " \n",
    "X_test = np.array(data_test['data'])\n",
    "Y_test = data_test['label']\n",
    "Y_test = np.array(Y_test)\n",
    "Y_test = Y_test.reshape(-1,1)\n",
    "print('number of samples: ', len(data_test['data']))\n",
    "print('keys: ', list(data_test.keys()))\n",
    "print('description: ', data_test['description'])\n",
    "print('image shape: ', data_test['data'][0].shape)\n",
    "print('labels:', np.unique(Y_test))\n",
    "\n",
    "Counter(data_test['label'])\n",
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "QQjzHd6TKkjH"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Conv2D, Input, ZeroPadding2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense,Dropout\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(100, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(100, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uqDHmxSaaBaq"
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('model-{epoch:03d}.model',monitor='val_loss',verbose=0,save_best_only=True,mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor= 'val_loss',mode ='min',verbose=1, patience = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orWL_21NPt_o",
    "outputId": "92b35cb5-863f-489b-8ac5-f4c500010242"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/60\n",
      "25/25 [==============================] - 139s 5s/step - loss: 6.3843 - acc: 0.4740 - val_loss: 0.6936 - val_acc: 0.3333\n",
      "Epoch 2/60\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.6911 - acc: 0.5410 - val_loss: 0.7321 - val_acc: 0.3384\n",
      "Epoch 3/60\n",
      "25/25 [==============================] - 122s 5s/step - loss: 0.8554 - acc: 0.6244 - val_loss: 0.5277 - val_acc: 0.8283\n",
      "Epoch 4/60\n",
      "25/25 [==============================] - 124s 5s/step - loss: 0.4117 - acc: 0.8448 - val_loss: 0.7644 - val_acc: 0.5455\n",
      "Epoch 5/60\n",
      "25/25 [==============================] - 127s 5s/step - loss: 0.2630 - acc: 0.9124 - val_loss: 0.2476 - val_acc: 0.8838\n",
      "Epoch 6/60\n",
      "25/25 [==============================] - 123s 5s/step - loss: 0.1467 - acc: 0.9514 - val_loss: 0.1852 - val_acc: 0.9444\n",
      "Epoch 7/60\n",
      "25/25 [==============================] - 113s 5s/step - loss: 0.2356 - acc: 0.9245 - val_loss: 0.1168 - val_acc: 0.9596\n",
      "Epoch 8/60\n",
      "25/25 [==============================] - 112s 4s/step - loss: 0.1162 - acc: 0.9595 - val_loss: 0.3158 - val_acc: 0.8586\n",
      "Epoch 9/60\n",
      "25/25 [==============================] - 114s 5s/step - loss: 0.2157 - acc: 0.9505 - val_loss: 0.6581 - val_acc: 0.8081\n",
      "Epoch 10/60\n",
      "25/25 [==============================] - 114s 5s/step - loss: 0.1007 - acc: 0.9548 - val_loss: 0.3588 - val_acc: 0.8737\n",
      "Epoch 11/60\n",
      "25/25 [==============================] - 115s 5s/step - loss: 0.4958 - acc: 0.9376 - val_loss: 0.1856 - val_acc: 0.9293\n",
      "Epoch 00011: early stopping\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x26fbc482c88>"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "model.fit(x=X_train, y=Y_train, validation_data=(X_test,Y_test), batch_size=25, epochs = 60,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-a15d9fea75d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m# Our operations on the frame come here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mgray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Display the resulting frame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-kh7iq4w7\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "'''import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Our operations on the frame come here\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('frame',gray)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict={0:'without_mask',1:'with_mask'}\n",
    "color_dict={0:(0,0,255),1:(0,255,0)}\n",
    "\n",
    "size = 4\n",
    "webcam = cv2.VideoCapture(0) #Use camera 0\n",
    "\n",
    "# We load the xml file\n",
    "classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    (rval, im) = webcam.read()\n",
    "    im=cv2.flip(im,1,1) #Flip to act as a mirror\n",
    "\n",
    "    # Resize the image to speed up detection\n",
    "    mini = cv2.resize(im, (im.shape[1] // size, im.shape[0] // size))\n",
    "\n",
    "    # detect MultiScale / faces \n",
    "    faces = classifier.detectMultiScale(mini)\n",
    "  #  model = load_model('model-021.model')\n",
    "    # Draw rectangles around each face\n",
    "    for f in faces:\n",
    "        (x, y, w, h) = [v * size for v in f] #Scale the shapesize backup\n",
    "        #Save just the rectangle faces in SubRecFaces\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(300,300))\n",
    "        reshaped=np.reshape(resized,(1,300,300,3))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "        #print(result)\n",
    "        \n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "    # Show the image\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "    # if Esc key is press then break out of the loop \n",
    "    if key == 27: #The Esc key\n",
    "        break\n",
    "# Stop video\n",
    "webcam.release()\n",
    "\n",
    "# Close all started windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MASK_DETECTION.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python368jvsc74a57bd0a54b4aca33c11560c67db02cf27fd91ac8e268774d0d2f50d2d73e32adaac664",
   "display_name": "Python 3.6.8 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "a54b4aca33c11560c67db02cf27fd91ac8e268774d0d2f50d2d73e32adaac664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}